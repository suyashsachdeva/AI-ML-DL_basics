{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Importing PyTorch libraries for neural networks\n",
    "from torchsummary import summary  # For summarizing the model architecture\n",
    "from torchvision import transforms  # For performing transformations on images\n",
    "from torch import utils  # Utilities for PyTorch like DataLoader and Tensor manipulation\n",
    "\n",
    "import cv2  # OpenCV for image processing tasks\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "import os  # For file and directory operations\n",
    "import random  # For generating random values\n",
    "\n",
    "from tqdm.auto import trange  # TQDM for progress bars in loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants for training configuration\n",
    "EPOCHS = 10              # Number of training epochs\n",
    "DATA_SIZE = 5000         # Total size of the training dataset\n",
    "BATCH_SIZE = 100         # Batch size for training\n",
    "VALID_DATA = 500         # Total size of the validation dataset\n",
    "VALID_BATCH = 100        # Batch size for validation\n",
    "IMG_SHAPE = 64           # Image dimensions (assumes square images of shape 64x64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, n_layers: int, filters: int, kernel: int = 3, growth_factor: float = 2.0, \n",
    "                 moment: float = 0.7, stride: bool = True, alpha: float = 0.03):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        PADDING = (kernel - 1) // 2  # Padding based on the kernel size\n",
    "        self.stride = stride\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.norm = nn.ModuleList([nn.BatchNorm1d(num_features=filters, momentum=moment) for _ in range(n_layers)])\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(filters, filters, kernel, padding=PADDING) for _ in range(n_layers - 1)])\n",
    "        \n",
    "        # Non-linear activation function\n",
    "        self.nlin = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Add final convolutional layer with stride or max pooling if applicable\n",
    "        if stride:\n",
    "            self.conv.append(nn.Conv2d(int(filters // growth_factor), filters, kernel, stride=2, padding=PADDING))\n",
    "        else:\n",
    "            self.conv.append(nn.Conv2d(int(filters // growth_factor), filters, kernel, padding=PADDING))\n",
    "            self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer\n",
    "\n",
    "        self.conv.reverse()  # Reverse the convolution layers for the forward pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.stride:\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Pass through convolutional and normalization layers with activation\n",
    "        for conv, norm in zip(self.conv, self.norm):\n",
    "            x = self.nlin(norm(conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShot(nn.Module):\n",
    "    def __init__(self, n_blocks: int = 7, n_high_refine: int = 3, n_conv_high_refine: int = 3, \n",
    "                 n_conv_end: int = 2, filters: int = 64, start_kernel: int = 5, kernel: int = 3, \n",
    "                 growth_factor: float = 2.0, alpha: float = 0.07, moment: float = 0.7, dense: int = 512, \n",
    "                 final: int = 100, drop: float = 0.2, stride: bool = True):\n",
    "        super(OneShot, self).__init__()\n",
    "\n",
    "        START_PADDING = (start_kernel - 1) // 2  # Padding for the starting convolution\n",
    "        PADDING = (kernel - 1) // 2  # Padding for regular convolution layers\n",
    "\n",
    "        # Initial convolutional layer with stride\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(3, filters, start_kernel, stride=2, padding=START_PADDING)])\n",
    "        \n",
    "        # Adding convolutional blocks\n",
    "        for c in range(n_blocks):\n",
    "            filters = int(filters * growth_factor)\n",
    "            if c <= n_high_refine:\n",
    "                # Add a high refinement ConvBlock\n",
    "                self.conv.append(ConvBlock(n_conv_high_refine, filters, kernel, growth_factor, moment, stride, alpha))\n",
    "            else:\n",
    "                # Add an end refinement ConvBlock\n",
    "                self.conv.append(ConvBlock(n_conv_end, filters, kernel, growth_factor, moment, stride, alpha))\n",
    "        \n",
    "        # Adaptive average pooling to reduce feature map size\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Flatten the output of the pooling layer\n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.lden = nn.Linear(int(filters * growth_factor), dense)\n",
    "        self.lvec = nn.Linear(dense, final)\n",
    "        \n",
    "        # Activation and dropout\n",
    "        self.nlin = nn.LeakyReLU(alpha)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through each convolutional block\n",
    "        for conv in self.conv:\n",
    "            x = conv(x)\n",
    "\n",
    "        # Pool, flatten, and apply the fully connected layers with dropout\n",
    "        x = self.flat(self.pool(x))\n",
    "        x = self.drop(self.nlin(self.lden))\n",
    "        return self.lvec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneShot()\n",
    "summary(model, (3, IMG_SHAPE, IMG_SHAPE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = r\"PATH\"  # Path to the dataset folder\n",
    "folders = os.listdir(FOLDER)  # List all subfolders (classes)\n",
    "train = []  # List to store triplet (anchor, positive, negative) samples\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(FOLDER + \"/\" + folder)  # List all files in the current folder\n",
    "\n",
    "    for i in range(len(files) - 1):\n",
    "        nfolder = random.choice(folders)  # Randomly select a different folder for the negative sample\n",
    "\n",
    "        if nfolder != folder:\n",
    "            path = r\"PATH\"  # Path for negative sample from another folder\n",
    "            negative = cv2.imread(path)  # Load negative image from a different folder\n",
    "        else:\n",
    "            path = r\"PATH\"  # Path for negative sample (you may want to change this logic)\n",
    "            negative = cv2.imread(path)  # Load negative image from the same folder\n",
    "\n",
    "        # Load the anchor and positive samples from the current folder\n",
    "        anchor = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[i])\n",
    "        positive = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[i + 1])\n",
    "\n",
    "        # Resize the images to the desired shape and normalize\n",
    "        anchor = cv2.resize(anchor, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "        positive = cv2.resize(positive, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "        negative = cv2.resize(negative, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "\n",
    "        # Append the triplet (anchor, positive, negative) to the training list\n",
    "        train.append([anchor, positive, negative])\n",
    "\n",
    "    # For folders with 4 or more files, create additional triplet samples\n",
    "    if len(files) >= 4:\n",
    "        for i in range(2):\n",
    "            if nfolder != folder:\n",
    "                fk = os.listdir(FOLDER + \"/\" + nfolder)[0]  # Select the first file from the negative folder\n",
    "                negative = cv2.imread(FOLDER + \"/\" + nfolder + \"/\" + fk)\n",
    "            else:\n",
    "                path = r\"PATH\"  # Path for the negative sample\n",
    "                negative = cv2.imread(path)\n",
    "\n",
    "            # Load the anchor and positive samples from the current folder\n",
    "            anchor = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[-2 + i])\n",
    "            positive = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[0 + i])\n",
    "\n",
    "            # Resize and normalize the images\n",
    "            anchor = cv2.resize(anchor, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "            positive = cv2.resize(positive, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "            negative = cv2.resize(negative, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "\n",
    "            # Append the triplet to the training list\n",
    "            train.append([anchor, positive, negative])\n",
    "\n",
    "# Convert the training list to a NumPy array\n",
    "x = np.array(train, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors for the training data (anchor, positive, negative)\n",
    "xtrain1 = torch.from_numpy(x[:DATA_SIZE, 0].reshape(DATA_SIZE, 3, IMG_SHAPE, IMG_SHAPE))  # Anchor images\n",
    "xtrain2 = torch.from_numpy(x[:DATA_SIZE, 1].reshape(DATA_SIZE, 3, IMG_SHAPE, IMG_SHAPE))  # Positive images\n",
    "xtrain3 = torch.from_numpy(x[:DATA_SIZE, 2].reshape(DATA_SIZE, 3, IMG_SHAPE, IMG_SHAPE))  # Negative images\n",
    "# ytrain = torch.from_numpy(y[:DATA_SIZE])  # Uncomment if you have labels (optional)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors for the validation data (anchor, positive, negative)\n",
    "xvalid1 = torch.from_numpy(x[DATA_SIZE:DATA_SIZE + VALID_DATA, 0].reshape(VALID_DATA, 3, IMG_SHAPE, IMG_SHAPE))  # Anchor images\n",
    "xvalid2 = torch.from_numpy(x[DATA_SIZE:DATA_SIZE + VALID_DATA, 1].reshape(VALID_DATA, 3, IMG_SHAPE, IMG_SHAPE))  # Positive images\n",
    "xvalid3 = torch.from_numpy(x[DATA_SIZE:DATA_SIZE + VALID_DATA, 2].reshape(VALID_DATA, 3, IMG_SHAPE, IMG_SHAPE))  # Negative images\n",
    "# yvalid = torch.from_numpy(y[DATA_SIZE:DATA_SIZE + VALID_DATA])  # Uncomment if you have labels (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader objects for training data (anchor, positive, negative)\n",
    "xtrain1 = utils.data.DataLoader(xtrain1, batch_size=BATCH_SIZE)  # DataLoader for anchor images in training\n",
    "xtrain2 = utils.data.DataLoader(xtrain2, batch_size=BATCH_SIZE)  # DataLoader for positive images in training\n",
    "xtrain3 = utils.data.DataLoader(xtrain3, batch_size=BATCH_SIZE)  # DataLoader for negative images in training\n",
    "# ytrain = utils.data.DataLoader(ytrain, batch_size=BATCH_SIZE)  # Uncomment if you have labels\n",
    "\n",
    "# Creating DataLoader objects for validation data (anchor, positive, negative)\n",
    "xvalid1 = utils.data.DataLoader(xvalid1, batch_size=VALID_BATCH)  # DataLoader for anchor images in validation\n",
    "xvalid2 = utils.data.DataLoader(xvalid2, batch_size=VALID_BATCH)  # DataLoader for positive images in validation\n",
    "xvalid3 = utils.data.DataLoader(xvalid3, batch_size=VALID_BATCH)  # DataLoader for negative images in validation\n",
    "# yvalid = utils.data.DataLoader(yvalid, batch_size=VALID_BATCH)  # Uncomment if you have labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of steps per epoch based on training and validation batches\n",
    "steps = len(xtrain1)  # Number of steps (batches) in training data\n",
    "vstep = len(xvalid1)  # Number of steps (batches) in validation data\n",
    "\n",
    "# Define the triplet loss function with pairwise distance\n",
    "criterion = nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance(), margin=10)\n",
    "\n",
    "# Set initial learning rate and decay factor for learning rate scheduling\n",
    "learning_rate = 1e-4\n",
    "decay = 0.9\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(EPOCHS):  # Iterate through epochs\n",
    "    lss = 0  # Initialize training loss for the epoch\n",
    "    vls = 0  # Initialize validation loss for the epoch\n",
    "    \n",
    "    # Decay learning rate over time\n",
    "    learning_rate = learning_rate / (1 + epoch * decay)\n",
    "    \n",
    "    # Optimizer with the updated learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop over batches\n",
    "    for step, (xtr1, xtr2, xtr3) in enumerate(zip(xtrain1, xtrain2, xtrain3)):\n",
    "        a = model(xtr1)  # Anchor output\n",
    "        p = model(xtr2)  # Positive output\n",
    "        n = model(xtr3)  # Negative output\n",
    "        \n",
    "        # Compute triplet loss\n",
    "        loss = criterion(a, p, n)\n",
    "        lss += loss  # Accumulate training loss\n",
    "        \n",
    "        # Zero the gradients, backpropagate, and update the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop over batches\n",
    "    for step, (xval1, xval2, xval3) in enumerate(zip(xvalid1, xvalid2, xvalid3)):\n",
    "        a = model(xval1)  # Anchor output for validation\n",
    "        p = model(xval2)  # Positive output for validation\n",
    "        n = model(xval3)  # Negative output for validation\n",
    "        \n",
    "        # Compute validation loss\n",
    "        valloss = criterion(a, p, n)\n",
    "        vls += valloss  # Accumulate validation loss\n",
    "    \n",
    "    # Print average training and validation loss for the current epoch\n",
    "    print(f'Epoch: {epoch+1}/{EPOCHS} || Loss: {lss/steps:.4f} || Validation Loss: {vls/vstep:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two images from the provided paths and preprocess them\n",
    "img1 = r\"PATH\"  # Path to the first image\n",
    "img2 = r\"PATH\"  # Path to the second image\n",
    "\n",
    "# Read, resize, and normalize image1, then convert it to a PyTorch tensor\n",
    "img1 = torch.from_numpy(np.array(cv2.resize(cv2.imread(img1), (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0, dtype=np.float32))\n",
    "\n",
    "# Read, resize, and normalize image2, then convert it to a PyTorch tensor\n",
    "img2 = torch.from_numpy(np.array(cv2.resize(cv2.imread(img2), (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0, dtype=np.float32))\n",
    "\n",
    "# Reshape the images to the format expected by the model: (batch_size, channels, height, width)\n",
    "pred1 = model(img1.reshape(1, 3, IMG_SHAPE, IMG_SHAPE))  # Predict features for image1\n",
    "pred2 = model(img2.reshape(1, 3, IMG_SHAPE, IMG_SHAPE))  # Predict features for image2\n",
    "\n",
    "# Calculate the pairwise distance between the two predicted feature vectors\n",
    "distance = torch.pairwise_distance(pred1, pred2)\n",
    "\n",
    "# Output the calculated distance\n",
    "print(distance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
