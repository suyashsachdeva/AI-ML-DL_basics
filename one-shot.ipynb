{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Feature Comparison using PyTorch\n",
    "\n",
    "This script demonstrates how to load, preprocess, and compare two images using a deep learning model in PyTorch. The main steps of the code are as follows:\n",
    "\n",
    "1. **Image Loading and Preprocessing**:\n",
    "    - Two images are loaded from the specified file paths using OpenCV (`cv2.imread`).\n",
    "    - The images are resized to the required shape using `cv2.resize` and normalized by scaling the pixel values between 0 and 1.\n",
    "    - The images are then converted into PyTorch tensors using `torch.from_numpy`, making them compatible with the neural network model.\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "    - The preprocessed images are reshaped to include a batch dimension (i.e., `(1, 3, IMG_SHAPE, IMG_SHAPE)`), which is necessary for passing the images through the model.\n",
    "    - The model takes each image as input and produces a feature vector that represents the image in a high-dimensional space.\n",
    "\n",
    "3. **Distance Calculation**:\n",
    "    - The pairwise distance between the feature vectors of the two images is computed using `torch.pairwise_distance`. This function calculates the Euclidean distance between the two feature vectors, indicating how similar or different the images are in the model's feature space.\n",
    "    \n",
    "The final output is the computed distance between the two images, which reflects their similarity according to the model's learned representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Importing PyTorch libraries for neural networks\n",
    "from torchsummary import summary  # For summarizing the model architecture\n",
    "from torchvision import transforms  # For performing transformations on images\n",
    "from torch import utils  # Utilities for PyTorch like DataLoader and Tensor manipulation\n",
    "\n",
    "import cv2  # OpenCV for image processing tasks\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "import os  # For file and directory operations\n",
    "import random  # For generating random values\n",
    "\n",
    "from tqdm.auto import trange  # TQDM for progress bars in loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants for training configuration\n",
    "EPOCHS = 10              # Number of training epochs\n",
    "DATA_SIZE = 5000         # Total size of the training dataset\n",
    "BATCH_SIZE = 100         # Batch size for training\n",
    "VALID_DATA = 500         # Total size of the validation dataset\n",
    "VALID_BATCH = 100        # Batch size for validation\n",
    "IMG_SHAPE = 64           # Image dimensions (assumes square images of shape 64x64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Descriptions\n",
    "\n",
    "### 1. **ConvBlock**\n",
    "\n",
    "The `ConvBlock` class represents a modular unit used in the `OneShot` architecture. Each block consists of several convolutional layers followed by batch normalization and non-linear activation using LeakyReLU. The block structure allows for both downsampling and maintaining feature resolution, depending on whether a stride or pooling operation is applied. \n",
    "\n",
    "Key Features:\n",
    "- **n_layers**: Defines the number of layers in the block.\n",
    "- **filters**: Number of convolutional filters per layer.\n",
    "- **kernel size**: Determines the receptive field of the convolution.\n",
    "- **growth_factor**: Controls the scaling of filters across layers.\n",
    "- **stride**: When `True`, a downsampling convolution is added, else max pooling is applied.\n",
    "- **LeakyReLU activation**: A non-linear activation function applied after each convolution and normalization.\n",
    "\n",
    "### 2. **OneShot Model**\n",
    "\n",
    "The `OneShot` model is a deep neural network designed to perform one-shot learning. It takes pairs or triplets of images as input and learns to map them to a feature space where similar images are closer together, while dissimilar images are far apart. The architecture is built from a sequence of `ConvBlock` layers that progressively extract more abstract features from the input images.\n",
    "\n",
    "Key Features:\n",
    "- **n_blocks**: The number of `ConvBlock` units used in the network.\n",
    "- **n_high_refine**: Specifies how many blocks use a higher refinement (more convolution layers) for feature extraction.\n",
    "- **filters**: The base number of filters used for the initial convolutional layers, which grows with each block according to the `growth_factor`.\n",
    "- **start_kernel**: Defines the kernel size for the first convolutional layer.\n",
    "- **dropout**: A dropout layer is used to reduce overfitting.\n",
    "- **AdaptiveAvgPool2d**: The network uses adaptive average pooling to reduce the final feature map size to (1,1) before flattening the features.\n",
    "- **Fully Connected Layers**: After the convolutional layers, a fully connected (dense) layer maps the output to the final dimensionality, representing the learned feature vector for the input image.\n",
    "\n",
    "### 3. **Triplet Loss**\n",
    "\n",
    "The model is trained using a **Triplet Margin Loss**, which works by comparing anchor, positive, and negative samples:\n",
    "- **Anchor**: The reference image.\n",
    "- **Positive**: An image of the same class or label as the anchor.\n",
    "- **Negative**: An image of a different class from the anchor.\n",
    "\n",
    "The goal is to minimize the distance between the anchor and the positive image, while maximizing the distance between the anchor and the negative image in the feature space. The margin ensures that the negative sample is sufficiently far from the anchor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, n_layers: int, filters: int, kernel: int = 3, growth_factor: float = 2.0, \n",
    "                 moment: float = 0.7, stride: bool = True, alpha: float = 0.03):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        PADDING = (kernel - 1) // 2  # Padding based on the kernel size\n",
    "        self.stride = stride\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.norm = nn.ModuleList([nn.BatchNorm1d(num_features=filters, momentum=moment) for _ in range(n_layers)])\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(filters, filters, kernel, padding=PADDING) for _ in range(n_layers - 1)])\n",
    "        \n",
    "        # Non-linear activation function\n",
    "        self.nlin = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Add final convolutional layer with stride or max pooling if applicable\n",
    "        if stride:\n",
    "            self.conv.append(nn.Conv2d(int(filters // growth_factor), filters, kernel, stride=2, padding=PADDING))\n",
    "        else:\n",
    "            self.conv.append(nn.Conv2d(int(filters // growth_factor), filters, kernel, padding=PADDING))\n",
    "            self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer\n",
    "\n",
    "        self.conv.reverse()  # Reverse the convolution layers for the forward pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.stride:\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Pass through convolutional and normalization layers with activation\n",
    "        for conv, norm in zip(self.conv, self.norm):\n",
    "            x = self.nlin(norm(conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShot(nn.Module):\n",
    "    def __init__(self, n_blocks: int = 7, n_high_refine: int = 3, n_conv_high_refine: int = 3, \n",
    "                 n_conv_end: int = 2, filters: int = 64, start_kernel: int = 5, kernel: int = 3, \n",
    "                 growth_factor: float = 2.0, alpha: float = 0.07, moment: float = 0.7, dense: int = 512, \n",
    "                 final: int = 100, drop: float = 0.2, stride: bool = True):\n",
    "        super(OneShot, self).__init__()\n",
    "\n",
    "        START_PADDING = (start_kernel - 1) // 2  # Padding for the starting convolution\n",
    "        PADDING = (kernel - 1) // 2  # Padding for regular convolution layers\n",
    "\n",
    "        # Initial convolutional layer with stride\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(3, filters, start_kernel, stride=2, padding=START_PADDING)])\n",
    "        \n",
    "        # Adding convolutional blocks\n",
    "        for c in range(n_blocks):\n",
    "            filters = int(filters * growth_factor)\n",
    "            if c <= n_high_refine:\n",
    "                # Add a high refinement ConvBlock\n",
    "                self.conv.append(ConvBlock(n_conv_high_refine, filters, kernel, growth_factor, moment, stride, alpha))\n",
    "            else:\n",
    "                # Add an end refinement ConvBlock\n",
    "                self.conv.append(ConvBlock(n_conv_end, filters, kernel, growth_factor, moment, stride, alpha))\n",
    "        \n",
    "        # Adaptive average pooling to reduce feature map size\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Flatten the output of the pooling layer\n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.lden = nn.Linear(int(filters * growth_factor), dense)\n",
    "        self.lvec = nn.Linear(dense, final)\n",
    "        \n",
    "        # Activation and dropout\n",
    "        self.nlin = nn.LeakyReLU(alpha)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through each convolutional block\n",
    "        for conv in self.conv:\n",
    "            x = conv(x)\n",
    "\n",
    "        # Pool, flatten, and apply the fully connected layers with dropout\n",
    "        x = self.flat(self.pool(x))\n",
    "        x = self.drop(self.nlin(self.lden))\n",
    "        return self.lvec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneShot()\n",
    "summary(model, (3, IMG_SHAPE, IMG_SHAPE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = r\"PATH\"  # Path to the dataset folder\n",
    "folders = os.listdir(FOLDER)  # List all subfolders (classes)\n",
    "train = []  # List to store triplet (anchor, positive, negative) samples\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(FOLDER + \"/\" + folder)  # List all files in the current folder\n",
    "\n",
    "    for i in range(len(files) - 1):\n",
    "        nfolder = random.choice(folders)  # Randomly select a different folder for the negative sample\n",
    "\n",
    "        if nfolder != folder:\n",
    "            path = r\"PATH\"  # Path for negative sample from another folder\n",
    "            negative = cv2.imread(path)  # Load negative image from a different folder\n",
    "        else:\n",
    "            path = r\"PATH\"  # Path for negative sample (you may want to change this logic)\n",
    "            negative = cv2.imread(path)  # Load negative image from the same folder\n",
    "\n",
    "        # Load the anchor and positive samples from the current folder\n",
    "        anchor = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[i])\n",
    "        positive = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[i + 1])\n",
    "\n",
    "        # Resize the images to the desired shape and normalize\n",
    "        anchor = cv2.resize(anchor, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "        positive = cv2.resize(positive, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "        negative = cv2.resize(negative, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "\n",
    "        # Append the triplet (anchor, positive, negative) to the training list\n",
    "        train.append([anchor, positive, negative])\n",
    "\n",
    "    # For folders with 4 or more files, create additional triplet samples\n",
    "    if len(files) >= 4:\n",
    "        for i in range(2):\n",
    "            if nfolder != folder:\n",
    "                fk = os.listdir(FOLDER + \"/\" + nfolder)[0]  # Select the first file from the negative folder\n",
    "                negative = cv2.imread(FOLDER + \"/\" + nfolder + \"/\" + fk)\n",
    "            else:\n",
    "                path = r\"PATH\"  # Path for the negative sample\n",
    "                negative = cv2.imread(path)\n",
    "\n",
    "            # Load the anchor and positive samples from the current folder\n",
    "            anchor = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[-2 + i])\n",
    "            positive = cv2.imread(FOLDER + \"/\" + folder + \"/\" + files[0 + i])\n",
    "\n",
    "            # Resize and normalize the images\n",
    "            anchor = cv2.resize(anchor, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "            positive = cv2.resize(positive, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "            negative = cv2.resize(negative, (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0\n",
    "\n",
    "            # Append the triplet to the training list\n",
    "            train.append([anchor, positive, negative])\n",
    "\n",
    "# Convert the training list to a NumPy array\n",
    "x = np.array(train, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors for the training data (anchor, positive, negative)\n",
    "xtrain1 = torch.from_numpy(x[:DATA_SIZE, 0].reshape(DATA_SIZE, 3, IMG_SHAPE, IMG_SHAPE))  # Anchor images\n",
    "xtrain2 = torch.from_numpy(x[:DATA_SIZE, 1].reshape(DATA_SIZE, 3, IMG_SHAPE, IMG_SHAPE))  # Positive images\n",
    "xtrain3 = torch.from_numpy(x[:DATA_SIZE, 2].reshape(DATA_SIZE, 3, IMG_SHAPE, IMG_SHAPE))  # Negative images\n",
    "# ytrain = torch.from_numpy(y[:DATA_SIZE])  # Uncomment if you have labels (optional)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors for the validation data (anchor, positive, negative)\n",
    "xvalid1 = torch.from_numpy(x[DATA_SIZE:DATA_SIZE + VALID_DATA, 0].reshape(VALID_DATA, 3, IMG_SHAPE, IMG_SHAPE))  # Anchor images\n",
    "xvalid2 = torch.from_numpy(x[DATA_SIZE:DATA_SIZE + VALID_DATA, 1].reshape(VALID_DATA, 3, IMG_SHAPE, IMG_SHAPE))  # Positive images\n",
    "xvalid3 = torch.from_numpy(x[DATA_SIZE:DATA_SIZE + VALID_DATA, 2].reshape(VALID_DATA, 3, IMG_SHAPE, IMG_SHAPE))  # Negative images\n",
    "# yvalid = torch.from_numpy(y[DATA_SIZE:DATA_SIZE + VALID_DATA])  # Uncomment if you have labels (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader objects for training data (anchor, positive, negative)\n",
    "xtrain1 = utils.data.DataLoader(xtrain1, batch_size=BATCH_SIZE)  # DataLoader for anchor images in training\n",
    "xtrain2 = utils.data.DataLoader(xtrain2, batch_size=BATCH_SIZE)  # DataLoader for positive images in training\n",
    "xtrain3 = utils.data.DataLoader(xtrain3, batch_size=BATCH_SIZE)  # DataLoader for negative images in training\n",
    "# ytrain = utils.data.DataLoader(ytrain, batch_size=BATCH_SIZE)  # Uncomment if you have labels\n",
    "\n",
    "# Creating DataLoader objects for validation data (anchor, positive, negative)\n",
    "xvalid1 = utils.data.DataLoader(xvalid1, batch_size=VALID_BATCH)  # DataLoader for anchor images in validation\n",
    "xvalid2 = utils.data.DataLoader(xvalid2, batch_size=VALID_BATCH)  # DataLoader for positive images in validation\n",
    "xvalid3 = utils.data.DataLoader(xvalid3, batch_size=VALID_BATCH)  # DataLoader for negative images in validation\n",
    "# yvalid = utils.data.DataLoader(yvalid, batch_size=VALID_BATCH)  # Uncomment if you have labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "\n",
    "### 1. **Data Preparation**\n",
    "The training process begins with the creation of **triplet data**:\n",
    "- **Anchor**: The reference image from a specific class.\n",
    "- **Positive**: An image of the same class as the anchor.\n",
    "- **Negative**: An image from a different class than the anchor.\n",
    "\n",
    "These triplets are loaded into PyTorch **DataLoader** objects to ensure efficient batching and iteration through the dataset.\n",
    "\n",
    "### 2. **Loss Function**\n",
    "The model is trained using **Triplet Margin Loss** with a pairwise distance function (`nn.PairwiseDistance()`). The objective is to minimize the distance between the anchor and the positive image while ensuring that the negative image is sufficiently distant. The margin is set to 10 in this case, meaning the negative sample should be at least this distance away from the anchor.\n",
    "\n",
    "### 3. **Optimizer and Learning Rate**\n",
    "The **Adam optimizer** is used for model optimization. The learning rate is initialized at `1e-4`, and **learning rate decay** is applied dynamically after each epoch using the following formula:\n",
    "\n",
    "learning_rate = learning_rate / (1 + epoch * decay)\n",
    "\n",
    "\n",
    "Where `decay = 0.9`. This progressively reduces the learning rate as training continues, helping the model converge smoothly.\n",
    "\n",
    "### 4. **Training Loop**\n",
    "For each epoch, the following operations are performed:\n",
    "\n",
    "- **Forward Pass**:\n",
    "    - For each batch, the triplet (anchor, positive, negative) is passed through the model.\n",
    "    - The model generates a feature vector for each image.\n",
    "    \n",
    "- **Loss Calculation**:\n",
    "    - The **Triplet Loss** is computed using the predicted feature vectors of the anchor, positive, and negative images.\n",
    "    - The total loss for the epoch is accumulated over all batches.\n",
    "\n",
    "- **Backpropagation and Optimization**:\n",
    "    - The model’s gradients are reset with `optimizer.zero_grad()`.\n",
    "    - The computed loss is backpropagated using `loss.backward()`.\n",
    "    - The optimizer updates the model’s weights using `optimizer.step()`.\n",
    "\n",
    "### 5. **Validation**\n",
    "After each training epoch, the model is evaluated on a separate validation set. The same triplet structure (anchor, positive, negative) is used to compute validation loss, providing a measure of the model's generalization to unseen data.\n",
    "\n",
    "### 6. **Performance Monitoring**\n",
    "For each epoch, the training and validation loss are printed to track the model's performance over time. This helps in ensuring the model is improving and not overfitting to the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of steps per epoch based on training and validation batches\n",
    "steps = len(xtrain1)  # Number of steps (batches) in training data\n",
    "vstep = len(xvalid1)  # Number of steps (batches) in validation data\n",
    "\n",
    "# Define the triplet loss function with pairwise distance\n",
    "criterion = nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance(), margin=10)\n",
    "\n",
    "# Set initial learning rate and decay factor for learning rate scheduling\n",
    "learning_rate = 1e-4\n",
    "decay = 0.9\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(EPOCHS):  # Iterate through epochs\n",
    "    lss = 0  # Initialize training loss for the epoch\n",
    "    vls = 0  # Initialize validation loss for the epoch\n",
    "    \n",
    "    # Decay learning rate over time\n",
    "    learning_rate = learning_rate / (1 + epoch * decay)\n",
    "    \n",
    "    # Optimizer with the updated learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop over batches\n",
    "    for step, (xtr1, xtr2, xtr3) in enumerate(zip(xtrain1, xtrain2, xtrain3)):\n",
    "        a = model(xtr1)  # Anchor output\n",
    "        p = model(xtr2)  # Positive output\n",
    "        n = model(xtr3)  # Negative output\n",
    "        \n",
    "        # Compute triplet loss\n",
    "        loss = criterion(a, p, n)\n",
    "        lss += loss  # Accumulate training loss\n",
    "        \n",
    "        # Zero the gradients, backpropagate, and update the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop over batches\n",
    "    for step, (xval1, xval2, xval3) in enumerate(zip(xvalid1, xvalid2, xvalid3)):\n",
    "        a = model(xval1)  # Anchor output for validation\n",
    "        p = model(xval2)  # Positive output for validation\n",
    "        n = model(xval3)  # Negative output for validation\n",
    "        \n",
    "        # Compute validation loss\n",
    "        valloss = criterion(a, p, n)\n",
    "        vls += valloss  # Accumulate validation loss\n",
    "    \n",
    "    # Print average training and validation loss for the current epoch\n",
    "    print(f'Epoch: {epoch+1}/{EPOCHS} || Loss: {lss/steps:.4f} || Validation Loss: {vls/vstep:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two images from the provided paths and preprocess them\n",
    "img1 = r\"PATH\"  # Path to the first image\n",
    "img2 = r\"PATH\"  # Path to the second image\n",
    "\n",
    "# Read, resize, and normalize image1, then convert it to a PyTorch tensor\n",
    "img1 = torch.from_numpy(np.array(cv2.resize(cv2.imread(img1), (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0, dtype=np.float32))\n",
    "\n",
    "# Read, resize, and normalize image2, then convert it to a PyTorch tensor\n",
    "img2 = torch.from_numpy(np.array(cv2.resize(cv2.imread(img2), (IMG_SHAPE, IMG_SHAPE), cv2.INTER_LINEAR) / 255.0, dtype=np.float32))\n",
    "\n",
    "# Reshape the images to the format expected by the model: (batch_size, channels, height, width)\n",
    "pred1 = model(img1.reshape(1, 3, IMG_SHAPE, IMG_SHAPE))  # Predict features for image1\n",
    "pred2 = model(img2.reshape(1, 3, IMG_SHAPE, IMG_SHAPE))  # Predict features for image2\n",
    "\n",
    "# Calculate the pairwise distance between the two predicted feature vectors\n",
    "distance = torch.pairwise_distance(pred1, pred2)\n",
    "\n",
    "# Output the calculated distance\n",
    "print(distance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
